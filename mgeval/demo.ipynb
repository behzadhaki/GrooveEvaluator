{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Demo : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"invalid normalization mode, return unnormalized matrix\")? (core.py, line 267)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/behzadhaki/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3343\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c9ffca72499e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from mgeval import core, utils\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/behzadhaki/Documents/School Work (Stored on Catalina and Mega Only)/GrooveEvaluator/mgeval/mgeval/core.py\"\u001b[0;36m, line \u001b[0;32m267\u001b[0m\n\u001b[0;31m    print \"invalid normalization mode, return unnormalized matrix\"\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"invalid normalization mode, return unnormalized matrix\")?\n"
     ]
    }
   ],
   "source": [
    "#import midi\n",
    "import glob\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mgeval import core, utils\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute measurement: statistic analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = glob.glob('data/output/*')\n",
    "print(set1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct empty dictionary to fill in measurement across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = 100\n",
    "num_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_eval = {'total_used_pitch':np.zeros((num_samples,1))}\n",
    "metrics_list = set1_eval.keys()\n",
    "for i in range(0, num_samples):\n",
    "    feature = core.extract_feature(set1[i])\n",
    "    set1_eval[metrics_list[0]][i] = getattr(core.metrics(), metrics_list[0])(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat for second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set2 = glob.glob('../data/set2/*.mid')\n",
    "set2 = glob.glob('data/baseline/*')\n",
    "set2_eval = {'total_used_pitch':np.zeros((num_samples,1))}\n",
    "for i in range(0, num_samples):\n",
    "    feature = core.extract_feature(set2[i])\n",
    "    set2_eval[metrics_list[0]][i] = getattr(core.metrics(), metrics_list[0])(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistic analysis: absolute measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(metrics_list)):\n",
    "    print metrics_list[i] + ':'\n",
    "    print '------------------------'\n",
    "    print ' demo_set'\n",
    "    print '  mean: ', np.mean(set1_eval[metrics_list[i]], axis=0)\n",
    "    print '  std: ', np.std(set1_eval[metrics_list[i]], axis=0)\n",
    "\n",
    "    print '------------------------'\n",
    "    print ' demo_set'\n",
    "    print '  mean: ', np.mean(set2_eval[metrics_list[i]], axis=0)\n",
    "    print '  std: ', np.std(set2_eval[metrics_list[i]], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative measurement: generalizes the result among features with various dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the features are sum- marized to \n",
    "- the intra-set distances\n",
    "- the difference of intra-set and inter-set distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exhaustive cross-validation for intra-set distances measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(np.arange(num_samples))\n",
    "set1_intra = np.zeros((num_samples, len(metrics_list), num_samples))\n",
    "set2_intra = np.zeros((num_samples, len(metrics_list), num_samples))\n",
    "for i in range(len(metrics_list)):\n",
    "    for train_index, test_index in loo.split(np.arange(num_samples)):\n",
    "        set1_intra[test_index[0]][i] = utils.c_dist(set1_eval[metrics_list[i]][test_index], set1_eval[metrics_list[i]][train_index])\n",
    "        set2_intra[test_index[0]][i] = utils.c_dist(set2_eval[metrics_list[i]][test_index], set2_eval[metrics_list[i]][train_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exhaustive cross-validation for inter-set distances measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(np.arange(num_samples))\n",
    "sets_inter = np.zeros((num_samples, len(metrics_list), num_samples))\n",
    "\n",
    "for i in range(len(metrics_list)):\n",
    "    for train_index, test_index in loo.split(np.arange(num_samples)):\n",
    "        sets_inter[test_index[0]][i] = utils.c_dist(set1_eval[metrics_list[i]][test_index], set2_eval[metrics_list[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of intra-set and inter-set distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_set1_intra = np.transpose(set1_intra,(1, 0, 2)).reshape(len(metrics_list), -1)\n",
    "plot_set2_intra = np.transpose(set2_intra,(1, 0, 2)).reshape(len(metrics_list), -1)\n",
    "plot_sets_inter = np.transpose(sets_inter,(1, 0, 2)).reshape(len(metrics_list), -1)\n",
    "for i in range(0,len(metrics_list)):\n",
    "    sns.kdeplot(plot_set1_intra[i], label='intra_set1')\n",
    "    sns.kdeplot(plot_sets_inter[i], label='inter')\n",
    "    sns.kdeplot(plot_set2_intra[i], label='intra_set2')\n",
    "\n",
    "    plt.title(metrics_list[i])\n",
    "    plt.xlabel('Euclidean distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference of intra-set and inter-set distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(metrics_list)):\n",
    "    print metrics_list[i] + ':'\n",
    "    print '------------------------'\n",
    "    print ' demo_set1'\n",
    "    print '  Kullback–Leibler divergence:',utils.kl_dist(plot_set1_intra[i], plot_sets_inter[i])\n",
    "    print '  Overlap area:', utils.overlap_area(plot_set1_intra[i], plot_sets_inter[i])\n",
    "    \n",
    "    print ' demo_set2'\n",
    "    print '  Kullback–Leibler divergence:',utils.kl_dist(plot_set2_intra[i], plot_sets_inter[i])\n",
    "    print '  Overlap area:', utils.overlap_area(plot_set2_intra[i], plot_sets_inter[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_thesis",
   "language": "python",
   "name": "torch_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
